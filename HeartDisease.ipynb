{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "import sklearn.linear_model\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and read files\n",
    "df_cleveland = pandas.read_csv(\"processed.cleveland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_hungarian = pandas.read_csv(\"processed.hungarian_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_switzerland = pandas.read_csv(\"processed.switzerland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_va = pandas.read_csv(\"processed.va_H.data\", sep=\",\",encoding='unicode_escape', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "df = [df_cleveland, df_hungarian, df_switzerland, df_va]\n",
    "full_df = pandas.concat(df)\n",
    "full_df.columns=['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal', 'diagnosed prediction']\n",
    "\n",
    "#we need the data as x and y where x is the data set and y is the target value\n",
    "#For y, we take the target value or diagnosed predictions so that any value greater than\n",
    "#0, is assigned 1. This means if there was no presence of heart disease, then\n",
    "#0 is assigned, and any presence (1-4) is assigned 1. This will help scale the \n",
    "#our predictions as this is a binary classification approach.\n",
    "\n",
    "#clean data\n",
    "#convert all data if numerical to float(except diagnoses prediction)\n",
    "#if data isn't numerical, make it NaN\n",
    "full_df = full_df.apply(pandas.to_numeric, errors='coerce')\n",
    "\n",
    "x = full_df.drop('diagnosed prediction', axis=1)\n",
    "y = full_df.drop(['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal'], axis=1)\n",
    "y['diagnosed prediction'].replace([1,2,3,4],[1,1,1,1],inplace=True)\n",
    "y_array = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into 70/15/15 as per literaure. Or 70% for training, 15% for validation, \n",
    "#15% for testing. From here, we will then impute and scale each set of data.\n",
    "#In total there will be 6x3=18 data sets. 6 will be trained, 6 will be used for\n",
    "#validation, and 6 will be used for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_array, test_size=0.3, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An issue with the data is that much of it, depending on its source\n",
    "#contains missing values denoted as \"?\". We took those values and\n",
    "#replaced them with \"NaN\". Now, we're going to look at multiple\n",
    "#cases where we attempt to find ways to work around these missing values\n",
    "\n",
    "#First, let's create a scaling function\n",
    "\n",
    "#Here we created a function named scaled which takes in x, a dataframe.\n",
    "#It will then scaled all the data between 0-1 as this works best for neural networks.\n",
    "#We will assign this function to every dataframe we intend on using (training/testing).\n",
    "def scale(x):\n",
    "    minimax_scaler = preprocessing.MinMaxScaler()\n",
    "    scaled = pandas.DataFrame(minimax_scaler.fit_transform(x.values), columns=x.columns, \n",
    "                              index=x.index)\n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are going to use k-NearestNeighbor(kNN) imputation, which is \n",
    "#a PVI to predict the missing values of each column. \n",
    "#We are going to use kNN on the data set with a k value set to 4.\n",
    "\n",
    "#first let's create a kNN imputation function\n",
    "def kNNimpute(x):\n",
    "    kNNimputer = KNNImputer(n_neighbors=4)\n",
    "    imputed = pandas.DataFrame(kNNimputer.fit_transform(x), columns=x.columns, index=x.index)\n",
    "    \n",
    "    return imputed\n",
    "\n",
    "df_train_data = kNNimpute(x_train)\n",
    "df_test_data = kNNimpute(x_test)\n",
    "df_val_data = kNNimpute(x_val)\n",
    "\n",
    "#scale\n",
    "df_train_scaled= scale(df_train_data)\n",
    "df_test_scaled= scale(df_test_data)\n",
    "df_val_scaled= scale(df_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_Layer:\n",
    "    \n",
    "    #initialize the layer\n",
    "    def __init__(self, neuron_inputs, neuron_neurons, L2_weight=0, L2_bias=0):\n",
    "        #initalize the variables\n",
    "        \n",
    "        #randomize the weights from a Gaussian Distribution with 0 as the\n",
    "        #mean and 1 as the variance\n",
    "        self.weights = 0.01 * np.random.randn(neuron_inputs, neuron_neurons)\n",
    "        self.biases = np.zeros((1, neuron_neurons))\n",
    "        \n",
    "        #initialize regularization for weight and bias\n",
    "        \n",
    "        self.L2_weight = L2_weight\n",
    "        self.L2_bias = L2_bias\n",
    "       \n",
    "        \n",
    "    #forward pass function for the first hidden layer                                  \n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        #saving the inputs for backwards pass\n",
    "        #the inputs of the forward pass is the input (data) itself\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #the output is the dot product of the inputs and weights \n",
    "        #plus the bias\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward_pass(self, dvalues):\n",
    "        \n",
    "        #calculate the gradients of weights, biases, and inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        \n",
    "        #calculate the gradient on L2 regularization (derivative  of L2)\n",
    "        \n",
    "        if self.L2_weight > 0:\n",
    "            self.dweights += 2*self.L2_weight*self.weights\n",
    "            \n",
    "        if self.L2_bias > 0:\n",
    "            self.dbiases += 2*self.L2_bias*self.biases\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #forward pass using the output from the hidden layer as it's input\n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "    \n",
    "        #the ReLU activation function will return 0 if the input is smaller\n",
    "        #than 0, and the actual input if greater than 0\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        #all derivatives greater than 0 are equal to themselves\n",
    "        self.dinputs[self.inputs > 0 ] = self.dinputs[self.inputs>0]\n",
    "        \n",
    "        #all derivatives less than 0 are equal to 0\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def predictions(self, outputs):\n",
    "        return np.round(outputs)\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #self.inputs = inputs\n",
    "        self.outputs = 1 / (1+np.exp(-inputs))\n",
    "        \n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues * (1-self.outputs) * self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def L2reg_loss(self, layer):\n",
    "        #initialize loss to 0\n",
    "        L2reg_loss = 0\n",
    "        \n",
    "        #changing the regularization loss if the weights \n",
    "        if layer.L2_weight >0:\n",
    "            L2reg_loss += layer.L2_weight* np.sum(layer.weights * layer.weights)\n",
    "        if layer.L2_bias >0:\n",
    "            L2reg_loss += layer.L2_bias*np.sum(layer.biases * layer.biases)\n",
    "                \n",
    "        return L2reg_loss\n",
    "                                              \n",
    "    \n",
    "    def calculate(self, outputs, y):\n",
    "        sample_loss = self.forward_pass(outputs, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class BinaryCrossEntropy(Loss):\n",
    "    def forward_pass(self, predicted_y, true_y):\n",
    "        predicted_y_clipped = np.clip(predicted_y, 1e-7, 1-1e-7)\n",
    "        sample_loss = -(true_y * np.log(predicted_y_clipped)+\n",
    "                        (1-true_y) * np.log(1-predicted_y_clipped))\n",
    "        sample_loss = np.mean(sample_loss, axis=-1)\n",
    "        return sample_loss\n",
    "    \n",
    "    def backward_pass(self, dvalues, true_y):\n",
    "        dvalues_clipped = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "        num_samples = len(dvalues)\n",
    "        num_outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -(true_y / dvalues_clipped - (1-true_y) / \\\n",
    "                         (1-dvalues_clipped)) / num_outputs\n",
    "        self.dinputs = self.dinputs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for optimizer. In this case we're using Adam\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0, decay=0, epsilon=0.00001, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon \n",
    "        self.b1 = b1 \n",
    "        self.b2 = b2\n",
    "        \n",
    "    #before we update parameters \n",
    "    def pre_parameter_update(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                        (1. / (1. + self.decay * self.iterations))\n",
    "    def parameter_update(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights) \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_momentum = np.zeros_like(layer.biases) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        #calculate momentum for the bias and weight using beta 1\n",
    "        layer.weight_momentum = self.b1 *  layer.weight_momentum + \\\n",
    "                                (1 - self.b1) * layer.dweights \n",
    "        layer.bias_momentum = self.b1 * layer.bias_momentum + \\\n",
    "                                (1 - self.b1) * layer.dbiases\n",
    "        #calculate cache for weight and bias using beta 2 (same as RMS prop)\n",
    "        layer.weight_cache = self.b2 * layer.weight_cache + \\\n",
    "                                (1 - self.b2) * (layer.dweights**2)\n",
    "        layer.bias_cache = self.b2 * layer.bias_cache + \\\n",
    "                                (1 - self.b2) * (layer.dbiases**2)\n",
    "    \n",
    "        #get corrected weight and bias momentum\n",
    "        weight_momentum_corrected = layer.weight_momentum / \\\n",
    "                                (1 - self.b1 ** (self.iterations + 1))\n",
    "        bias_momentum_corrected = layer.bias_momentum / \\\n",
    "                                (1 - self.b1 ** (self.iterations + 1))\n",
    "        \n",
    "        #get corrected weight and bias cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                (1 - self.b2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                                (1 - self.b2 ** (self.iterations + 1))\n",
    "        \n",
    "        #now we update the weights and bias\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                                weight_momentum_corrected / (np.sqrt(weight_cache_corrected) +\n",
    "                                                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                                bias_momentum_corrected / (np.sqrt(bias_cache_corrected) +\n",
    "                                                           self.epsilon)                 \n",
    "        \n",
    "    def post_parameter_update(self): \n",
    "        self.iterations = self.iterations + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1-rate\n",
    "    def forward_pass(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        self.outputs = inputs * self.binary_mask\n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.550, loss: 0.693, data_loss: 0.693, reg_loss: 0.001, lr: 0.001\n",
      "epoch: 5, acc: 0.548, loss: 0.686, data_loss: 0.686, reg_loss: 0.001, lr: 0.0009999960000160002\n",
      "epoch: 10, acc: 0.548, loss: 0.676, data_loss: 0.676, reg_loss: 0.001, lr: 0.0009999910000809994\n",
      "epoch: 15, acc: 0.548, loss: 0.664, data_loss: 0.664, reg_loss: 0.001, lr: 0.0009999860001959973\n",
      "epoch: 20, acc: 0.554, loss: 0.649, data_loss: 0.649, reg_loss: 0.002, lr: 0.0009999810003609932\n",
      "epoch: 25, acc: 0.581, loss: 0.631, data_loss: 0.631, reg_loss: 0.003, lr: 0.0009999760005759862\n",
      "epoch: 30, acc: 0.649, loss: 0.612, data_loss: 0.612, reg_loss: 0.003, lr: 0.0009999710008409756\n",
      "epoch: 35, acc: 0.713, loss: 0.590, data_loss: 0.590, reg_loss: 0.005, lr: 0.0009999660011559607\n",
      "epoch: 40, acc: 0.761, loss: 0.564, data_loss: 0.564, reg_loss: 0.006, lr: 0.0009999610015209409\n",
      "epoch: 45, acc: 0.775, loss: 0.541, data_loss: 0.541, reg_loss: 0.008, lr: 0.000999956001935915\n",
      "epoch: 50, acc: 0.786, loss: 0.517, data_loss: 0.517, reg_loss: 0.009, lr: 0.0009999510024008825\n",
      "epoch: 55, acc: 0.789, loss: 0.499, data_loss: 0.499, reg_loss: 0.011, lr: 0.0009999460029158425\n",
      "epoch: 60, acc: 0.789, loss: 0.481, data_loss: 0.481, reg_loss: 0.013, lr: 0.0009999410034807947\n",
      "epoch: 65, acc: 0.797, loss: 0.468, data_loss: 0.468, reg_loss: 0.015, lr: 0.0009999360040957377\n",
      "epoch: 70, acc: 0.801, loss: 0.461, data_loss: 0.461, reg_loss: 0.016, lr: 0.0009999310047606714\n",
      "epoch: 75, acc: 0.811, loss: 0.449, data_loss: 0.449, reg_loss: 0.018, lr: 0.000999926005475595\n",
      "validation, acc: 0.833,  loss: 0.447\n",
      "test, acc: 0.862,  loss: 0.419\n"
     ]
    }
   ],
   "source": [
    "#create a hidden layer. Given 13 features we will produce 14 outputs(neurons)\n",
    "first_hidden = Hidden_Layer(13, 270, L2_weight=0.002, L2_bias=0.002)\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "#print(y_train)\n",
    "\n",
    "#initialize ReLU activation\n",
    "ReLU_activation = ReLU()\n",
    "\n",
    "first_dropout = Dropout(0.2)\n",
    "\n",
    "#create a second hidden layer\n",
    "second_hidden = Hidden_Layer(270, 1)\n",
    "\n",
    "#initialize sigmoid\n",
    "Sigmoid_activation = Sigmoid()\n",
    "\n",
    "#initialize loss function\n",
    "BinaryCrossEntropyloss = BinaryCrossEntropy()\n",
    "\n",
    "#initialize optimizing function\n",
    "Adam_optimizer = Adam(learning_rate=0.001, decay=0.000001)\n",
    "\n",
    "\n",
    "for epoch in range(80):\n",
    "\n",
    "        #we perform the forward pass on the training data\n",
    "        first_hidden.forward_pass(df_train_scaled)\n",
    "\n",
    "        #forward pass the outputs of the first layer to ReLU\n",
    "        ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "        \n",
    "        first_dropout.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "        #forward pass the outputs of ReLU in the second layer\n",
    "        second_hidden.forward_pass(first_dropout.outputs)\n",
    "        \n",
    "        #forward pass the outputs of the second hidden layer to sigmoid\n",
    "        Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "        \n",
    "        #calculate loss on forward pass\n",
    "        loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_train)\n",
    "        \n",
    "        #calculate regularization loss\n",
    "    \n",
    "        \n",
    "        L2reg_loss = \\\n",
    "            BinaryCrossEntropyloss.L2reg_loss(first_hidden) + BinaryCrossEntropyloss.L2reg_loss(second_hidden)\n",
    "        total_loss = loss + L2reg_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        #compare outputs from softmax to y for accuracy\n",
    "        probabilities =  Sigmoid_activation.outputs\n",
    "        predictions = np.round(probabilities)\n",
    "        \n",
    "        #predictions = (Sigmoid_activation.outputs > 0.5) *  1\n",
    "        accuracy = np.mean(predictions == y_train)\n",
    "        \n",
    "        if not epoch % 5: \n",
    "            print(f'epoch: {epoch}, ' +\n",
    "                  f'acc: {accuracy:.3f}, ' +\n",
    "                  f'loss: {loss:.3f}, ' +\n",
    "                  f'data_loss: {loss:.3f}, ' +\n",
    "                  f'reg_loss: {L2reg_loss:.3f}, ' +\n",
    "                  f'lr: {Adam_optimizer.current_learning_rate}')\n",
    "            \n",
    "        #THE CHAIN RULE APPLIED HERE!!!\n",
    "\n",
    "        #backwards pass the loss_activation output through the loss activation backward\n",
    "        BinaryCrossEntropyloss.backward_pass(Sigmoid_activation.outputs, y_train)\n",
    "\n",
    "        Sigmoid_activation.backward_pass(BinaryCrossEntropyloss.dinputs)\n",
    "\n",
    "        #backwards pass the derivative of the loss activation inputs through\n",
    "        #the backward function of dense2\n",
    "        second_hidden.backward_pass(Sigmoid_activation.dinputs)\n",
    "        \n",
    "        first_dropout.backward_pass(second_hidden.dinputs)\n",
    "\n",
    "\n",
    "        #backwards pass the derivative of the dense2 derivative inputs\n",
    "        #through the backward function of first activation function\n",
    "        ReLU_activation.backward_pass(first_dropout.dinputs)\n",
    "     \n",
    "        #backwards pass the derivative of the first activation function inputs\n",
    "        #through the backwards function of dense1\n",
    "        first_hidden.backward_pass(ReLU_activation.dinputs)\n",
    "            \n",
    "        \n",
    "        Adam_optimizer.pre_parameter_update()\n",
    "        Adam_optimizer.parameter_update(first_hidden)\n",
    "        Adam_optimizer.parameter_update(second_hidden) \n",
    "        Adam_optimizer.post_parameter_update()\n",
    "        \n",
    "\n",
    "y_val = y_val.reshape(-1,1) \n",
    "\n",
    "first_hidden.forward_pass(df_val_scaled)\n",
    "\n",
    "ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "\n",
    "second_hidden.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_val)\n",
    "\n",
    "#compare outputs from softmax to y for accuracy\n",
    "probabilities =  Sigmoid_activation.outputs\n",
    "predictions = np.round(probabilities)\n",
    "\n",
    "accuracy = np.mean(predictions == y_val)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f},  loss: {loss:.3f}')\n",
    "\n",
    "\n",
    "y_test = y_test.reshape(-1,1) \n",
    "\n",
    "first_hidden.forward_pass(df_test_scaled)\n",
    "\n",
    "ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "\n",
    "second_hidden.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_test)\n",
    "\n",
    "#compare outputs from softmax to y for accuracy\n",
    "probabilities =  Sigmoid_activation.outputs\n",
    "predictions = np.round(probabilities)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'test, acc: {accuracy:.3f},  loss: {loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heart_disease",
   "language": "python",
   "name": "heart_disease"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
