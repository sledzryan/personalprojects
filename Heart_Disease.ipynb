{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "import sklearn.linear_model\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and read files\n",
    "df_cleveland = pandas.read_csv(\"processed.cleveland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_hungarian = pandas.read_csv(\"processed.hungarian_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_switzerland = pandas.read_csv(\"processed.switzerland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_va = pandas.read_csv(\"processed.va_H.data\", sep=\",\",encoding='unicode_escape', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1607,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "df = [df_cleveland, df_hungarian, df_switzerland, df_va]\n",
    "full_df = pandas.concat(df)\n",
    "full_df.columns=['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal', 'diagnosed prediction']\n",
    "\n",
    "#we need the data as x and y where x is the data set and y is the target value\n",
    "#For y, we take the target value or diagnosed predictions so that any value greater than\n",
    "#0, is assigned 1. This means if there was no presence of heart disease, then\n",
    "#0 is assigned, and any presence (1-4) is assigned 1. This will help scale the \n",
    "#our predictions as this is a binary classification approach.\n",
    "\n",
    "#clean data\n",
    "#convert all data if numerical to float(except diagnoses prediction)\n",
    "#if data isn't numerical, make it NaN\n",
    "full_df = full_df.apply(pandas.to_numeric, errors='coerce')\n",
    "\n",
    "x = full_df.drop('diagnosed prediction', axis=1)\n",
    "y = full_df.drop(['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal'], axis=1)\n",
    "y['diagnosed prediction'].replace([1,2,3,4],[1,1,1,1],inplace=True)\n",
    "y_array = y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1608,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plan on having 6 different cases. To avoid a data leak, the plan is to\n",
    "#split the data into 70/15/15 as per literaure. Or 70% for training, 15% for validation, \n",
    "#15% for testing. From here, we will then impute and scale each set of data.\n",
    "#In total there will be 6x3=18 data sets. 6 will be trained, 6 will be used for\n",
    "#validation, and 6 will be used for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_array, test_size=0.3, random_state=50)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1609,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An issue with the data is that much of it, depending on its source\n",
    "#contains missing values denoted as \"?\". We took those values and\n",
    "#replaced them with \"NaN\". Now, we're going to look at multiple\n",
    "#cases where we attempt to find ways to work around these missing values\n",
    "\n",
    "#First, let's create a scaling function\n",
    "\n",
    "#Here we created a function named scaled which takes in x, a dataframe.\n",
    "#It will then scaled all the data between 0-1 as this works best for neural networks.\n",
    "#We will assign this function to every dataframe we intend on using (training/testing).\n",
    "def scale(x):\n",
    "    minimax_scaler = preprocessing.MinMaxScaler()\n",
    "    scaled = pandas.DataFrame(minimax_scaler.fit_transform(x.values), columns=x.columns, \n",
    "                              index=x.index)\n",
    "    return scaled\n",
    "\n",
    "#Case 1: Now that we have separated the data, we are going to impute it.\n",
    "#Our first imputition technique will setting all NaN values to 0.\n",
    "#We are also going to scale the data between 0-1. This range works best \n",
    "#for neural networks\n",
    "df1_train_data = x_train.fillna(0)\n",
    "df1_test_data = x_test.fillna(0)\n",
    "df1_val_data = x_val.fillna(0)\n",
    "\n",
    "#Now, let's scale each set of data.\n",
    "df1_train_scaled= scale(df1_train_data)\n",
    "df1_test_scaled= scale(df1_test_data)\n",
    "df1_val_scaled= scale(df1_val_data)\n",
    "\n",
    "#Now that we have split, imputed, and scaled the data. Let's do the same for the rest of\n",
    "#our cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 1610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case 2: Our second imputation technique is to delete all instances \n",
    "#with NaN values, this cuts our number of instances in third\n",
    "df2_train_data = x_train.dropna()\n",
    "df2_test_data = x_test.dropna()\n",
    "df2_val_data = x_val.dropna()\n",
    "\n",
    "\n",
    "#scale\n",
    "df2_train_scaled= scale(df2_train_data)\n",
    "df2_test_scaled= scale(df2_test_data)\n",
    "df2_val_scaled= scale(df2_val_data)\n",
    "len(df2_train_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1611,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 3: Here we are going to delete all features with NaN value\n",
    "#cuts our number of features roughly in fourth\n",
    "df3_train_data = x_train.dropna(axis=1)\n",
    "df3_test_data = x_test.dropna(axis=1)\n",
    "df3_val_data = x_val.dropna(axis=1)\n",
    "\n",
    "\n",
    "#scale\n",
    "df3_train_scaled= scale(df3_train_data)\n",
    "df3_test_scaled= scale(df3_test_data)\n",
    "df3_val_scaled= scale(df3_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1612,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 4: Here we are using a Predictive Value Imputation (PVI) which will \n",
    "#take the individual mean of columns containing missing values and replace\n",
    "#the NaN, or missing values with the mean\n",
    "df4_train_data= x_train.fillna(x_train.mean())\n",
    "df4_test_data= x_test.fillna(x_test.mean())\n",
    "df4_val_data= x_val.fillna(x_val.mean())\n",
    "\n",
    "\n",
    "#scale\n",
    "df4_train_scaled= scale(df4_train_data)\n",
    "df4_test_scaled= scale(df4_test_data)\n",
    "df4_val_scaled= scale(df4_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1613,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 5: Here we are using a Predictive Value Imputation (PVI) which will \n",
    "#take the individual mean of columns containing missing values and replace\n",
    "#the NaN, or missing values with the median\n",
    "df5_train_data = x_train.fillna(x_train.median())\n",
    "df5_test_data = x_test.fillna(x_test.median())\n",
    "df5_val_data = x_val.fillna(x_val.median())\n",
    "\n",
    "#scale\n",
    "df5_train_scaled= scale(df5_train_data)\n",
    "df5_test_scaled= scale(df5_test_data)\n",
    "df5_val_scaled= scale(df5_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 1614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case 6: Here we are going to use k-NearestNeighbor(kNN) imputation, which is \n",
    "#another PVI to predict the missing values of each column. \n",
    "#We are going to use kNN on the data set with a k value set to 4.\n",
    "\n",
    "#first let's create a kNN imputation function\n",
    "def kNNimpute(x):\n",
    "    kNNimputer = KNNImputer(n_neighbors=4)\n",
    "    imputed = pandas.DataFrame(kNNimputer.fit_transform(x), columns=x.columns, index=x.index)\n",
    "    \n",
    "    return imputed\n",
    "\n",
    "df6_train_data = kNNimpute(x_train)\n",
    "df6_test_data = kNNimpute(x_test)\n",
    "df6_val_data = kNNimpute(x_val)\n",
    "\n",
    "#scale\n",
    "df6_train_scaled= scale(df6_train_data)\n",
    "df6_test_scaled= scale(df6_test_data)\n",
    "df6_val_scaled= scale(df6_val_data)\n",
    "len(df6_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1615,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that our data has been split into train, test, and validate into\n",
    "#ratios of 70/15/15 respectively, we can begin building our model\n",
    "\n",
    "#The layout for the neural network is as follows:\n",
    "\n",
    "#Forward Pass:\n",
    "\n",
    "#Our input layer will be all scaled x values. They will be assigned \n",
    "#random weights which will be multiplied by each x.\n",
    "#All x's at every neuron in the first dense (hidden) layer will \n",
    "#be summed and have a bias added to them. From there, we now have\n",
    "#output values from the input layer in the neurons of the \n",
    "#first hidden layer. Because we summed different weights multiplied\n",
    "#by x, and then added them to a bias. Each input in the first hidden\n",
    "#layer will contain a different value. Here, we will use our first \n",
    "#activation function. With the goal of creating the best model for\n",
    "#this problem in mind, we are going to use two commonly chosen \n",
    "#acitvation functions for this layer: ReLU and Leaky ReLU.\n",
    "\n",
    "#ReLU will return either 0, if the value produced is less than or \n",
    "#equal to 0, or it will return the value itself if it greater than 0.\n",
    "\n",
    "#Leaky ReLU is used to combat the dying ReLU problem. This occurs when\n",
    "#many of the outputs of ReLU are negative, creating a bias. It can\n",
    "#also occur when the learning rate is too high. Leaky ReLU will return the\n",
    "#value itself if it's greater than 0. If it is equal to 0, it will return\n",
    "#0, and if it is less than 0, the slope of the function will tell us that\n",
    "#it will return a number barely, but increasingly smaller than 0. \n",
    "#This is because the slope is very small, but not zero. \n",
    "\n",
    "#For the second hidden layer, we wil use the Sigmoid function. This will\n",
    "#allow us to return probabilities in the output layer.\n",
    "\n",
    "#Loss:\n",
    "\n",
    "#For loss, we will use cross-entropy loss function. This will be applied \n",
    "#to the second hidden layer with respect to the predictions, y.\n",
    "#We will also use L2 or ridge regularization to calculate the loss on \n",
    "#both hidden layers. This is the loss calculated from our optimizer.\n",
    "#The total amount of loss will be the product of both types of loss.\n",
    "\n",
    "#Backpropogation:\n",
    "\n",
    "#For backpropogation, we need to apply the chain rule to every layer.\n",
    "#For this, we first start with the derivative of the BinaryCrossEntropy\n",
    "#with respect to the sigmoid activation output. Then, we take the \n",
    "#derivative of the sigmoid function with respect to the derivative\n",
    "#of the inputs of BinaryCrossEntropy. Next, we take the derivative\n",
    "#of the values of the input of sigmoid function. Then we take the \n",
    "#derivative of ReLU with respect to the derivative of the inputs \n",
    "#of the second hidden layer. We will then take the derivative of \n",
    "#the values and L2 regularization of weights and biases with respect to\n",
    "#the input of the derivative of ReLU.\n",
    "\n",
    "#We will then update our hyperparameters which are the weights and biases\n",
    "#along with the learning rate from our optimizer, Adam. This will be done\n",
    "#until we recieve an acceptable loss on our training, validation, and \n",
    "#testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1616,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_Layer:\n",
    "    \n",
    "    #initialize the layer\n",
    "    def __init__(self, neuron_inputs, neuron_neurons, L2_weight=0, L2_bias=0):\n",
    "        #initalize the variables\n",
    "        \n",
    "        #randomize the weights from a Gaussian Distribution with 0 as the\n",
    "        #mean and 1 as the variance\n",
    "        self.weights = 0.01 * np.random.randn(neuron_inputs, neuron_neurons)\n",
    "        self.biases = np.zeros((1, neuron_neurons))\n",
    "        \n",
    "        #initialize regularization for weight and bias\n",
    "        \n",
    "        self.L2_weight = L2_weight\n",
    "        self.L2_bias = L2_bias\n",
    "       \n",
    "        \n",
    "    #forward pass function for the first hidden layer                                  \n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        #saving the inputs for backwards pass\n",
    "        #the inputs of the forward pass is the input (data) itself\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #the output is the dot product of the inputs and weights \n",
    "        #plus the bias\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward_pass(self, dvalues):\n",
    "        \n",
    "        #calculate the gradients of weights, biases, and inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        \n",
    "        #calculate the gradient on L2 regularization (derivative  of L2)\n",
    "        \n",
    "        if self.L2_weight > 0:\n",
    "            self.dweights += 2*self.L2_weight*self.weights\n",
    "            \n",
    "        if self.L2_bias > 0:\n",
    "            self.dbiases += 2*self.L2_bias*self.biases\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #forward pass using the output from the hidden layer as it's input\n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "    \n",
    "        #the ReLU activation function will return 0 if the input is smaller\n",
    "        #than 0, and the actual input if greater than 0\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward_pass(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = 1 / (1+np.exp(-inputs))\n",
    "        \n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues * (1-self.outputs) * self.outputs\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1619,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def L2reg_loss(self, layer):\n",
    "        #initialize loss to 0\n",
    "        L2reg_loss = 0\n",
    "        \n",
    "        #changing the regularization loss if the weights \n",
    "        if layer.L2_weight >0:\n",
    "            L2reg_loss += layer.L2_weight* np.sum(layer.weights * layer.weights)\n",
    "        \n",
    "        if layer.L2_bias >0:\n",
    "            L2reg_loss += layer.L2_bias*np.sum(layer.biases * layer.biases)\n",
    "            \n",
    "        return L2reg_loss\n",
    "                                              \n",
    "    \n",
    "    def calculate(self, outputs, y):\n",
    "        sample_loss = self.forward_pass(outputs, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "    \n",
    "class BinaryCrossEntropy(Loss):\n",
    "    def forward_pass(self, predicted_y, true_y):\n",
    "        predicted_y_clipped = np.clip(predicted_y, 1e-7, 1-1e-7)\n",
    "        #print(predicted_y_clipped)\n",
    "        sample_loss = -(true_y * np.log(predicted_y_clipped)+\n",
    "                        (1-true_y) * np.log(1-predicted_y_clipped))\n",
    "        sample_loss = np.mean(sample_loss, axis=-1)\n",
    "        return sample_loss\n",
    "    \n",
    "    def backward_pass(self, dvalues, true_y):\n",
    "        dvalues_clipped = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "        num_samples = len(dvalues)\n",
    "        num_outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -(true_y / dvalues_clipped - (1-true_y) / \\\n",
    "                         (1-dvalues_clipped)) / num_outputs\n",
    "        self.dinputs = self.dinputs / num_samples\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1620,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for optimizer. In this case we're using Adam\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0, decay=0, epsilon=1e-7, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon \n",
    "        self.b1 = b1 \n",
    "        self.b2 = b2\n",
    "        \n",
    "    #before we update parameters \n",
    "    def pre_parameter_update(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                        (1. / (1. + self.decay * self.iterations))\n",
    "    def parameter_update(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights) \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_momentum = np.zeros_like(layer.biases) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        #calculate momentum for the bias and weight using beta 1\n",
    "        layer.weight_momentum = self.b1 *  layer.weight_momentum + \\\n",
    "                                (1 - self.b1) * layer.dweights \n",
    "        layer.bias_momentum = self.b1 * layer.bias_momentum + \\\n",
    "                                (1 - self.b1) * layer.dbiases\n",
    "        \n",
    "        #calculate cache for weight and bias using beta 2 (same as RMS prop)\n",
    "        layer.weight_cache = self.b2 * layer.weight_cache + \\\n",
    "                                (1 - self.b2) * (layer.dweights**2)\n",
    "        layer.bias_cache = self.b2 * layer.bias_cache + \\\n",
    "                                (1 - self.b2) * (layer.dbiases**2)\n",
    "    \n",
    "        #get corrected weight and bias momentum\n",
    "        weight_momentum_corrected = layer.weight_momentum / \\\n",
    "                                (1 - self.b1 ** (self.iterations + 1))\n",
    "        bias_momentum_corrected = layer.bias_momentum / \\\n",
    "                                (1 - self.b1 ** (self.iterations + 1))\n",
    "        \n",
    "        #get corrected weight and bias cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                (1 - self.b2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                                (1 - self.b2 ** (self.iterations + 1))\n",
    "        \n",
    "        #now we update the weights and bias\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                                weight_momentum_corrected / (np.sqrt(weight_cache_corrected) +\n",
    "                                                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                                bias_momentum_corrected / (np.sqrt(bias_cache_corrected) +\n",
    "                                                           self.epsilon)                 \n",
    "        \n",
    "    def post_parameter_update(self): \n",
    "        self.iterations = self.iterations + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1621,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1-rate\n",
    "    def forward_pass(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        self.outputs = inputs * self.binary_mask\n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.545, loss: 0.693, data_loss: 0.693, reg_loss: 0.000, lr: 0.0015\n",
      "epoch: 20, acc: 0.634, loss: 0.605, data_loss: 0.605, reg_loss: 0.002, lr: 0.0014943215780035865\n",
      "epoch: 40, acc: 0.793, loss: 0.474, data_loss: 0.474, reg_loss: 0.008, lr: 0.001488390553681286\n",
      "epoch: 60, acc: 0.814, loss: 0.433, data_loss: 0.433, reg_loss: 0.012, lr: 0.0014825064241945048\n",
      "epoch: 80, acc: 0.811, loss: 0.423, data_loss: 0.423, reg_loss: 0.012, lr: 0.0014766686355581807\n",
      "epoch: 100, acc: 0.814, loss: 0.421, data_loss: 0.421, reg_loss: 0.012, lr: 0.0014708766424789174\n",
      "epoch: 120, acc: 0.817, loss: 0.414, data_loss: 0.414, reg_loss: 0.012, lr: 0.0014651299081851923\n",
      "epoch: 140, acc: 0.815, loss: 0.415, data_loss: 0.415, reg_loss: 0.012, lr: 0.0014594279042615295\n",
      "epoch: 160, acc: 0.823, loss: 0.411, data_loss: 0.411, reg_loss: 0.012, lr: 0.0014537701104865284\n",
      "epoch: 180, acc: 0.821, loss: 0.412, data_loss: 0.412, reg_loss: 0.011, lr: 0.0014481560146746475\n",
      "epoch: 200, acc: 0.828, loss: 0.408, data_loss: 0.408, reg_loss: 0.011, lr: 0.0014425851125216388\n",
      "epoch: 220, acc: 0.815, loss: 0.404, data_loss: 0.404, reg_loss: 0.011, lr: 0.0014370569074535352\n",
      "epoch: 240, acc: 0.820, loss: 0.399, data_loss: 0.399, reg_loss: 0.011, lr: 0.001431570910479099\n",
      "epoch: 260, acc: 0.820, loss: 0.398, data_loss: 0.398, reg_loss: 0.012, lr: 0.001426126640045636\n",
      "epoch: 280, acc: 0.823, loss: 0.393, data_loss: 0.393, reg_loss: 0.012, lr: 0.0014207236218980867\n",
      "validation, acc: 0.877,  loss: 0.385\n"
     ]
    }
   ],
   "source": [
    "#create a hidden layer. Given 13 features we will produce 14 outputs(neurons)\n",
    "first_hidden = Hidden_Layer(13, 330, L2_weight=0.001, L2_bias=0.001)\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "#print(y_train)\n",
    "\n",
    "#initialize ReLU activation\n",
    "ReLU_activation = ReLU()\n",
    "\n",
    "first_dropout = Dropout(0.2)\n",
    "\n",
    "#create a second hidden layer\n",
    "second_hidden = Hidden_Layer(330, 1)\n",
    "\n",
    "\n",
    "#initialize sigmoid\n",
    "Sigmoid_activation = Sigmoid()\n",
    "\n",
    "#initialize loss function\n",
    "BinaryCrossEntropyloss = BinaryCrossEntropy()\n",
    "\n",
    "#initialize optimizing function\n",
    "Adam_optimizer = Adam(learning_rate=0.0015, decay=0.0002)\n",
    "\n",
    "for epoch in range(300):\n",
    "\n",
    "        #we perform the forward pass on the training data\n",
    "        first_hidden.forward_pass(df6_train_scaled)\n",
    "\n",
    "        #forward pass the outputs of the first layer to ReLU\n",
    "        ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "        \n",
    "        first_dropout.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "        #forward pass the outputs of ReLU in the second layer\n",
    "        second_hidden.forward_pass(first_dropout.outputs)\n",
    "        \n",
    "        #forward pass the outputs of the second hidden layer to sigmoid\n",
    "        Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "        \n",
    "        #calculate loss on forward pass\n",
    "        loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_train)\n",
    "        \n",
    "        #calculate regularization loss\n",
    "    \n",
    "        \n",
    "        L2reg_loss = \\\n",
    "            BinaryCrossEntropyloss.L2reg_loss(first_hidden) + \\\n",
    "            BinaryCrossEntropyloss.L2reg_loss(second_hidden)\n",
    "        total_loss = loss + L2reg_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        #compare outputs from softmax to y for accuracy\n",
    "        probabilities =  Sigmoid_activation.outputs\n",
    "        predictions = np.round(probabilities)\n",
    "        \n",
    "        #predictions = (Sigmoid_activation.outputs > 0.5) *  1\n",
    "        accuracy = np.mean(predictions == y_train)\n",
    "        \n",
    "        if not epoch % 20: \n",
    "            print(f'epoch: {epoch}, ' +\n",
    "                  f'acc: {accuracy:.3f}, ' +\n",
    "                  f'loss: {loss:.3f}, ' +\n",
    "                  f'data_loss: {loss:.3f}, ' +\n",
    "                  f'reg_loss: {L2reg_loss:.3f}, ' +\n",
    "                  f'lr: {Adam_optimizer.current_learning_rate}')\n",
    "            \n",
    "        #THE CHAIN RULE APPLIED HERE!!!\n",
    "\n",
    "        #backwards pass the loss_activation output through the loss activation backward\n",
    "        BinaryCrossEntropyloss.backward_pass(Sigmoid_activation.outputs, y_train)\n",
    "\n",
    "        Sigmoid_activation.backward_pass(BinaryCrossEntropyloss.dinputs)\n",
    "\n",
    "        #backwards pass the derivative of the loss activation inputs through\n",
    "        #the backward function of dense2\n",
    "        second_hidden.backward_pass(Sigmoid_activation.dinputs)\n",
    "        \n",
    "        first_dropout.backward_pass(second_hidden.dinputs)\n",
    "\n",
    "\n",
    "        #backwards pass the derivative of the dense2 derivative inputs\n",
    "        #through the backward function of first activation function\n",
    "        ReLU_activation.backward_pass(first_dropout.dinputs)\n",
    "     \n",
    "        #backwards pass the derivative of the first activation function inputs\n",
    "        #through the backwards function of dense1\n",
    "        first_hidden.backward_pass(ReLU_activation.dinputs)\n",
    "            \n",
    "        \n",
    "        Adam_optimizer.pre_parameter_update()\n",
    "        Adam_optimizer.parameter_update(first_hidden)\n",
    "        Adam_optimizer.parameter_update(second_hidden) \n",
    "        Adam_optimizer.post_parameter_update()\n",
    "        \n",
    "\n",
    "y_val = y_val.reshape(-1,1) \n",
    "\n",
    "first_hidden.forward_pass(df6_val_scaled)\n",
    "\n",
    "ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "\n",
    "second_hidden.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "\n",
    "#Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "\n",
    "Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_val)\n",
    "\n",
    "#compare outputs from softmax to y for accuracy\n",
    "probabilities =  Sigmoid_activation.outputs\n",
    "predictions = np.round(probabilities)\n",
    "\n",
    "accuracy = np.mean(predictions == y_val)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f},  loss: {loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
