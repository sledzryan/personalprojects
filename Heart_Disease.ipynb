{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "import sklearn.linear_model\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and read files\n",
    "df_cleveland = pandas.read_csv(\"processed.cleveland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_hungarian = pandas.read_csv(\"processed.hungarian_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_switzerland = pandas.read_csv(\"processed.switzerland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_va = pandas.read_csv(\"processed.va_H.data\", sep=\",\",encoding='unicode_escape', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "df = [df_cleveland, df_hungarian, df_switzerland, df_va]\n",
    "full_df = pandas.concat(df)\n",
    "full_df.columns=['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal', 'diagnosed prediction']\n",
    "\n",
    "#we need the data as x and y where x is the data set and y is the target value\n",
    "#For y, we take the target value or diagnosed predictions so that any value greater than\n",
    "#0, is assigned 1. This means if there was no presence of heart disease, then\n",
    "#0 is assigned, and any presence (1-4) is assigned 1. This will help scale the \n",
    "#our predictions as this is a binary classification approach.\n",
    "\n",
    "#clean data\n",
    "#convert all data if numerical to float(except diagnoses prediction)\n",
    "#if data isn't numerical, make it NaN\n",
    "full_df = full_df.apply(pandas.to_numeric, errors='coerce')\n",
    "\n",
    "x = full_df.drop('diagnosed prediction', axis=1)\n",
    "y = full_df.drop(['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal'], axis=1)\n",
    "y['diagnosed prediction'].replace([1,2,3,4],[1,1,1,1],inplace=True)\n",
    "y_array = y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#We plan on having 6 different cases. To avoid a data leak, the plan is to\n",
    "#split the data into 70/15/15 as per literaure. Or 70% for training, 15% for validation, \n",
    "#15% for testing. From here, we will then impute and scale each set of data.\n",
    "#In total there will be 6x3=18 data sets. 6 will be trained, 6 will be used for\n",
    "#validation, and 6 will be used for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_array, test_size=0.3, random_state=40)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=40)\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An issue with the data is that much of it, depending on its source\n",
    "#contains missing values denoted as \"?\". We took those values and\n",
    "#replaced them with \"NaN\". Now, we're going to look at multiple\n",
    "#cases where we attempt to find ways to work around these missing values\n",
    "\n",
    "#First, let's create a scaling function\n",
    "\n",
    "#Here we created a function named scaled which takes in x, a dataframe.\n",
    "#It will then scaled all the data between 0-1 as this works best for neural networks.\n",
    "#We will assign this function to every dataframe we intend on using (training/testing).\n",
    "def scale(x):\n",
    "    minimax_scaler = preprocessing.MinMaxScaler()\n",
    "    scaled = pandas.DataFrame(minimax_scaler.fit_transform(x.values), columns=x.columns, \n",
    "                              index=x.index)\n",
    "    return scaled\n",
    "\n",
    "#Case 1: Now that we have separated the data, we are going to impute it.\n",
    "#Our first imputition technique will setting all NaN values to 0.\n",
    "#We are also going to scale the data between 0-1. This range works best \n",
    "#for neural networks\n",
    "df1_train_data = x_train.fillna(0)\n",
    "df1_test_data = x_test.fillna(0)\n",
    "df1_val_data = x_val.fillna(0)\n",
    "\n",
    "#Now, let's scale each set of data.\n",
    "df1_train_scaled= scale(df1_train_data)\n",
    "df1_test_scaled= scale(df1_test_data)\n",
    "df1_val_scaled= scale(df1_val_data)\n",
    "\n",
    "#Now that we have split, imputed, and scaled the data. Let's do the same for the rest of\n",
    "#our cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 2: Our second imputation technique is to delete all instances \n",
    "#with NaN values, this cuts our number of instances in third\n",
    "df2_train_data = x_train.dropna()\n",
    "df2_test_data = x_test.dropna()\n",
    "df2_val_data = x_val.dropna()\n",
    "\n",
    "\n",
    "#scale\n",
    "df2_train_scaled= scale(df2_train_data)\n",
    "df2_test_scaled= scale(df2_test_data)\n",
    "df2_val_scaled= scale(df2_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 3: Here we are going to delete all features with NaN value\n",
    "#cuts our number of features roughly in fourth\n",
    "df3_train_data = x_train.dropna(axis=1)\n",
    "df3_test_data = x_test.dropna(axis=1)\n",
    "df3_val_data = x_val.dropna(axis=1)\n",
    "\n",
    "\n",
    "#scale\n",
    "df3_train_scaled= scale(df3_train_data)\n",
    "df3_test_scaled= scale(df3_test_data)\n",
    "df3_val_scaled= scale(df3_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 4: Here we are using a Predictive Value Imputation (PVI) which will \n",
    "#take the individual mean of columns containing missing values and replace\n",
    "#the NaN, or missing values with the mean\n",
    "df4_train_data= x_train.fillna(x_train.mean())\n",
    "df4_test_data= x_test.fillna(x_test.mean())\n",
    "df4_val_data= x_val.fillna(x_val.mean())\n",
    "\n",
    "\n",
    "#scale\n",
    "df4_train_scaled= scale(df4_train_data)\n",
    "df4_test_scaled= scale(df4_test_data)\n",
    "df4_val_scaled= scale(df4_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 5: Here we are using a Predictive Value Imputation (PVI) which will \n",
    "#take the individual mean of columns containing missing values and replace\n",
    "#the NaN, or missing values with the median\n",
    "df5_train_data = x_train.fillna(x_train.median())\n",
    "df5_test_data = x_test.fillna(x_test.median())\n",
    "df5_val_data = x_val.fillna(x_val.median())\n",
    "\n",
    "#scale\n",
    "df5_train_scaled= scale(df5_train_data)\n",
    "df5_test_scaled= scale(df5_test_data)\n",
    "df5_val_scaled= scale(df5_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 6: Here we are going to use k-NearestNeighbor(kNN) imputation, which is \n",
    "#another PVI to predict the missing values of each column. \n",
    "#We are going to use kNN on the data set with a k value set to 4.\n",
    "\n",
    "#first let's create a kNN imputation function\n",
    "def kNNimpute(x):\n",
    "    kNNimputer = KNNImputer(n_neighbors=4)\n",
    "    imputed = pandas.DataFrame(kNNimputer.fit_transform(x), columns=x.columns, index=x.index)\n",
    "    \n",
    "    return imputed\n",
    "\n",
    "df6_train_data = kNNimpute(x_train)\n",
    "df6_test_data = kNNimpute(x_test)\n",
    "df6_val_data = kNNimpute(x_val)\n",
    "\n",
    "#scale\n",
    "df6_train_scaled= scale(df6_train_data)\n",
    "df6_test_scaled= scale(df6_test_data)\n",
    "df6_val_scaled= scale(df6_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that our data has been split into train, test, and validate into\n",
    "#ratios of 70/15/15 respectively, we can begin building our model\n",
    "\n",
    "#The layout for the neural network is as follows:\n",
    "\n",
    "#Forward Pass:\n",
    "\n",
    "#Our input layer will be all scaled x values. They will be assigned \n",
    "#random weights which will be multiplied by each x.\n",
    "#All x's at every neuron in the first dense (hidden) layer will \n",
    "#be summed and have a bias added to them. From there, we now have\n",
    "#output values from the input layer in the neurons of the \n",
    "#first hidden layer. Because we summed different weights multiplied\n",
    "#by x, and then added them to a bias. Each input in the first hidden\n",
    "#layer will contain a different value. Here, we will use our first \n",
    "#activation function. With the goal of creating the best model for\n",
    "#this problem in mind, we are going to use two commonly chosen \n",
    "#acitvation functions for this layer: ReLU and Leaky ReLU.\n",
    "\n",
    "#ReLU will return either 0, if the value produced is less than or \n",
    "#equal to 0, or it will return the value itself if it greater than 0.\n",
    "\n",
    "#Leaky ReLU is used to combat the dying ReLU problem. This occurs when\n",
    "#many of the outputs of ReLU are negative, creating a bias. It can\n",
    "#also occur when the learning rate is too high. Leaky ReLU will return the\n",
    "#value itself if it's greater than 0. If it is equal to 0, it will return\n",
    "#0, and if it is less than 0, the slope of the function will tell us that\n",
    "#it will return a number barely, but increasingly smaller than 0. \n",
    "#This is because the slope is very small, but not zero. \n",
    "\n",
    "#For the second hidden layer, we wil use the Sigmoid function. This will\n",
    "#allow us to return probabilities in the output layer.\n",
    "\n",
    "#Loss:\n",
    "\n",
    "#For loss, we will use cross-entropy loss function. This will be applied \n",
    "#to the second hidden layer with respect to the predictions, y.\n",
    "#We will also use L2 or ridge regularization to calculate the loss on \n",
    "#both hidden layers. This is the loss calculated from our optimizer.\n",
    "#The total amount of loss will be the product of both types of loss.\n",
    "\n",
    "#Backpropogation:\n",
    "\n",
    "#For backpropogation, we need to apply the chain rule to every layer.\n",
    "#For this, we first start with the derivative of the BinaryCrossEntropy\n",
    "#with respect to the sigmoid activation output. Then, we take the \n",
    "#derivative of the sigmoid function with respect to the derivative\n",
    "#of the inputs of BinaryCrossEntropy. Next, we take the derivative\n",
    "#of the values of the input of sigmoid function. Then we take the \n",
    "#derivative of ReLU with respect to the derivative of the inputs \n",
    "#of the second hidden layer. We will then take the derivative of \n",
    "#the values and L2 regularization of weights and biases with respect to\n",
    "#the input of the derivative of ReLU.\n",
    "\n",
    "#We will then update our hyperparameters which are the weights and biases\n",
    "#along with the learning rate from our optimizer, Adam. This will be done\n",
    "#until we recieve an acceptable loss on our training, validation, and \n",
    "#testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_Layer:\n",
    "    \n",
    "    #initialize the layer\n",
    "    def __init__(self, neuron_inputs, neuron_neurons):\n",
    "        #initalize the variables\n",
    "        \n",
    "        #randomize the weights from a Gaussian Distribution with 0 as the\n",
    "        #mean and 1 as the variance\n",
    "        self.weights = 0.01 * np.random.randn(neuron_inputs, neuron_neurons)\n",
    "        self.biases = np.zeros((1, neuron_neurons))\n",
    "        \n",
    "    #forward pass function for the first hidden layer                                  \n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        #saving the inputs for backwards pass\n",
    "        #the inputs of the forward pass is the input (data) itself\n",
    "        #self.inputs = inputs\n",
    "        \n",
    "        #the output is the dot product of the inputs and weights \n",
    "        #plus the bias\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #forward pass using the output from the hidden layer as it's input\n",
    "    def forward_pass(self, inputs):\n",
    "    \n",
    "        #the ReLU activation function will return 0 if the input is smaller\n",
    "        #than 0, and the actual input if greater than 0\n",
    "        self.outputs = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    #forward pass to create normalized probabilities for output\n",
    "    def forward_pass(self, inputs):\n",
    "        #e to the inputs of every input\n",
    "        #subtract every input by the max first to get a very small number\n",
    "        #this is done to avoid exploding neurons\n",
    "        unnormal = np.exp(inputs - np.max(inputs, axis=0, keepdims=False))\n",
    "        #normalize the probabilities be dividing them by the sum\n",
    "        #of all unnormal probabilities\n",
    "        #set axis =1 to get values as a column \n",
    "        self.outputs = unnormal / np.sum(unnormal, axis= 0, keepdims=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward_pass(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "    \n",
    "class BinaryCrossEntropy(Loss):\n",
    "    def forward_pass(self, predicted_y, true_y):\n",
    "        predicted_y_clipped = np.clip(predicted_y, 1e-7, 1-1e-7)\n",
    "        #print(predicted_y_clipped)\n",
    "        sample_loss = -(true_y * np.log(predicted_y_clipped)+\n",
    "                        (1-true_y) * np.log(1-predicted_y_clipped))\n",
    "        sample_loss = np.mean(sample_loss, axis=1)\n",
    "        return sample_loss\n",
    "    \n",
    "    def backward_pass(self, dvalues, true_y):\n",
    "        dvalues_clipped = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "        num_samples = len(dvalues)\n",
    "        num_outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -(true_y / dvalues_clipped - (1-true_y) / \\\n",
    "                         (1-dvalues))\n",
    "        self.dinputs = self.dinputs / num_samples\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  3.7367904237948015\n",
      "predictions:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "acc:  0.422360248447205\n"
     ]
    }
   ],
   "source": [
    "#create a hidden layer. Given 13 features we will produce 14 outputs(neurons)\n",
    "first_hidden = Hidden_Layer(13, 14)     \n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "#print(y_train)\n",
    "\n",
    "#initialize ReLU activation\n",
    "ReLU_activation = ReLU()\n",
    "\n",
    "#create a second hidden layer\n",
    "second_hidden = Hidden_Layer(14, 1)\n",
    "\n",
    "#initialize softmax\n",
    "Softmax_activation = Softmax()\n",
    "\n",
    "#initalize loss function\n",
    "BinaryCrossEntropyloss = BinaryCrossEntropy()\n",
    "\n",
    "#we perform the forward pass on the training data\n",
    "first_hidden.forward_pass(df6_train_scaled)\n",
    "\n",
    "#forward pass the outputs of the first layer to ReLU\n",
    "ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "\n",
    "#forward pass the outputs of ReLU in the second layer\n",
    "second_hidden.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "#forward pass the ouputs of the second hidden layer to softmax\n",
    "Softmax_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "loss = BinaryCrossEntropyloss.calculate(Softmax_activation.outputs, y_train)\n",
    "print('loss: ', loss)\n",
    "\n",
    "\n",
    "probabilities =  Softmax_activation.outputs\n",
    "\n",
    "predictions = np.round(probabilities)\n",
    "\n",
    "print('predictions: ', predictions)\n",
    "\n",
    "accuracy = np.mean(predictions == y_train)\n",
    "      #Print accuracy\n",
    "print('acc: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
