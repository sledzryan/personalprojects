{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "import sklearn.linear_model\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and read files\n",
    "df_cleveland = pandas.read_csv(\"processed.cleveland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_hungarian = pandas.read_csv(\"processed.hungarian_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_switzerland = pandas.read_csv(\"processed.switzerland_H.data\", sep=\",\",encoding='unicode_escape', header=None)\n",
    "df_va = pandas.read_csv(\"processed.va_H.data\", sep=\",\",encoding='unicode_escape', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "df = [df_cleveland, df_hungarian, df_switzerland, df_va]\n",
    "full_df = pandas.concat(df)\n",
    "full_df.columns=['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal', 'diagnosed prediction']\n",
    "\n",
    "#we need the data as x and y where x is the data set and y is the target value\n",
    "#For y, we take the target value or diagnosed predictions so that any value greater than\n",
    "#0, is assigned 1. This means if there was no presence of heart disease, then\n",
    "#0 is assigned, and any presence (1-4) is assigned 1. This will help scale the \n",
    "#our predictions as this is a binary classification approach.\n",
    "\n",
    "#clean data\n",
    "#convert all data if numerical to float(except diagnoses prediction)\n",
    "#if data isn't numerical, make it NaN\n",
    "full_df = full_df.apply(pandas.to_numeric, errors='coerce')\n",
    "\n",
    "x = full_df.drop('diagnosed prediction', axis=1)\n",
    "y = full_df.drop(['age','sex', 'chest pain type', 'resting bp(mm/Hg)', \n",
    "                 'serum cholesterol(mg/dl)', 'fasting bs', 'resting ecg',\n",
    "                'max hr', 'ex induced angina', 'ST depression', 'slope',\n",
    "                'nmv', 'thal'], axis=1)\n",
    "y['diagnosed prediction'].replace([1,2,3,4],[1,1,1,1],inplace=True)\n",
    "y_array = y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest pain type</th>\n",
       "      <th>resting bp(mm/Hg)</th>\n",
       "      <th>serum cholesterol(mg/dl)</th>\n",
       "      <th>fasting bs</th>\n",
       "      <th>resting ecg</th>\n",
       "      <th>max hr</th>\n",
       "      <th>ex induced angina</th>\n",
       "      <th>ST depression</th>\n",
       "      <th>slope</th>\n",
       "      <th>nmv</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  chest pain type  resting bp(mm/Hg)  serum cholesterol(mg/dl)  \\\n",
       "60   43.0  0.0              2.0              120.0                     201.0   \n",
       "178  59.0  0.0              2.0              130.0                     188.0   \n",
       "53   55.0  1.0              4.0              120.0                       0.0   \n",
       "180  59.0  1.0              3.0              130.0                     318.0   \n",
       "52   42.0  0.0              3.0              115.0                     211.0   \n",
       "..    ...  ...              ...                ...                       ...   \n",
       "106  59.0  1.0              4.0              140.0                     177.0   \n",
       "270  61.0  1.0              4.0              140.0                     207.0   \n",
       "140  75.0  1.0              4.0              160.0                     310.0   \n",
       "132  53.0  0.0              2.0              140.0                     216.0   \n",
       "102  57.0  0.0              4.0              128.0                     303.0   \n",
       "\n",
       "     fasting bs  resting ecg  max hr  ex induced angina  ST depression  slope  \\\n",
       "60          0.0          0.0   165.0                0.0            0.0    NaN   \n",
       "178         0.0          0.0   124.0                0.0            1.0    2.0   \n",
       "53          0.0          1.0    92.0                0.0            0.3    1.0   \n",
       "180         0.0          0.0   120.0                1.0            1.0    2.0   \n",
       "52          0.0          1.0   137.0                0.0            0.0    NaN   \n",
       "..          ...          ...     ...                ...            ...    ...   \n",
       "106         0.0          0.0   162.0                1.0            0.0    1.0   \n",
       "270         0.0          2.0   138.0                1.0            1.9    1.0   \n",
       "140         1.0          0.0   112.0                1.0            2.0    3.0   \n",
       "132         0.0          0.0   142.0                1.0            2.0    2.0   \n",
       "102         0.0          2.0   159.0                0.0            0.0    1.0   \n",
       "\n",
       "     nmv  thal  \n",
       "60   NaN   NaN  \n",
       "178  NaN   NaN  \n",
       "53   NaN   7.0  \n",
       "180  NaN   3.0  \n",
       "52   NaN   NaN  \n",
       "..   ...   ...  \n",
       "106  1.0   7.0  \n",
       "270  1.0   7.0  \n",
       "140  NaN   7.0  \n",
       "132  NaN   NaN  \n",
       "102  1.0   3.0  \n",
       "\n",
       "[644 rows x 13 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We plan on having 6 different cases. To avoid a data leak, the plan is to\n",
    "#split the data into 70/15/15 as per literaure. Or 70% for training, 15% for validation, \n",
    "#15% for testing. From here, we will then impute and scale each set of data.\n",
    "#In total there will be 6x3=18 data sets. 6 will be trained, 6 will be used for\n",
    "#validation, and 6 will be used for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_array, test_size=0.3, random_state=50)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=50)\n",
    "\n",
    "x_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An issue with the data is that much of it, depending on its source\n",
    "#contains missing values denoted as \"?\". We took those values and\n",
    "#replaced them with \"NaN\". Now, we're going to look at multiple\n",
    "#cases where we attempt to find ways to work around these missing values\n",
    "\n",
    "#First, let's create a scaling function\n",
    "\n",
    "#Here we created a function named scaled which takes in x, a dataframe.\n",
    "#It will then scaled all the data between 0-1 as this works best for neural networks.\n",
    "#We will assign this function to every dataframe we intend on using (training/testing).\n",
    "def scale(x):\n",
    "    minimax_scaler = preprocessing.MinMaxScaler()\n",
    "    scaled = pandas.DataFrame(minimax_scaler.fit_transform(x.values), columns=x.columns, \n",
    "                              index=x.index)\n",
    "    return scaled\n",
    "\n",
    "#Case 1: Now that we have separated the data, we are going to impute it.\n",
    "#Our first imputition technique will setting all NaN values to 0.\n",
    "#We are also going to scale the data between 0-1. This range works best \n",
    "#for neural networks\n",
    "df1_train_data = x_train.fillna(0)\n",
    "df1_test_data = x_test.fillna(0)\n",
    "df1_val_data = x_val.fillna(0)\n",
    "\n",
    "#Now, let's scale each set of data.\n",
    "df1_train_scaled= scale(df1_train_data)\n",
    "df1_test_scaled= scale(df1_test_data)\n",
    "df1_val_scaled= scale(df1_val_data)\n",
    "\n",
    "#Now that we have split, imputed, and scaled the data. Let's do the same for the rest of\n",
    "#our cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 6: Here we are going to use k-NearestNeighbor(kNN) imputation, which is \n",
    "#another PVI to predict the missing values of each column. \n",
    "#We are going to use kNN on the data set with a k value set to 4.\n",
    "\n",
    "#first let's create a kNN imputation function\n",
    "def kNNimpute(x):\n",
    "    kNNimputer = KNNImputer(n_neighbors=4)\n",
    "    imputed = pandas.DataFrame(kNNimputer.fit_transform(x), columns=x.columns, index=x.index)\n",
    "    \n",
    "    return imputed\n",
    "\n",
    "df6_train_data = kNNimpute(x_train)\n",
    "df6_test_data = kNNimpute(x_test)\n",
    "df6_val_data = kNNimpute(x_val)\n",
    "\n",
    "#scale\n",
    "df6_train_scaled= scale(df6_train_data)\n",
    "df6_test_scaled= scale(df6_test_data)\n",
    "df6_val_scaled= scale(df6_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that our data has been split into train, test, and validate into\n",
    "#ratios of 70/15/15 respectively, we can begin building our model\n",
    "\n",
    "#The layout for the neural network is as follows:\n",
    "\n",
    "#Forward Pass:\n",
    "\n",
    "#Our input layer will be all scaled x values. They will be assigned \n",
    "#random weights which will be multiplied by each x.\n",
    "#All x's at every neuron in the first dense (hidden) layer will \n",
    "#be summed and have a bias added to them. From there, we now have\n",
    "#output values from the input layer in the neurons of the \n",
    "#first hidden layer. Because we summed different weights multiplied\n",
    "#by x, and then added them to a bias. Each input in the first hidden\n",
    "#layer will contain a different value. Here, we will use our first \n",
    "#activation function. With the goal of creating the best model for\n",
    "#this problem in mind, we are going to use two commonly chosen \n",
    "#acitvation functions for this layer: ReLU and Leaky ReLU.\n",
    "\n",
    "#ReLU will return either 0, if the value produced is less than or \n",
    "#equal to 0, or it will return the value itself if it greater than 0.\n",
    "\n",
    "#Leaky ReLU is used to combat the dying ReLU problem. This occurs when\n",
    "#many of the outputs of ReLU are negative, creating a bias. It can\n",
    "#also occur when the learning rate is too high. Leaky ReLU will return the\n",
    "#value itself if it's greater than 0. If it is equal to 0, it will return\n",
    "#0, and if it is less than 0, the slope of the function will tell us that\n",
    "#it will return a number barely, but increasingly smaller than 0. \n",
    "#This is because the slope is very small, but not zero. \n",
    "\n",
    "#For the second hidden layer, we wil use the Sigmoid function. This will\n",
    "#allow us to return probabilities in the output layer.\n",
    "\n",
    "#Loss:\n",
    "\n",
    "#For loss, we will use cross-entropy loss function. This will be applied \n",
    "#to the second hidden layer with respect to the predictions, y.\n",
    "#We will also use L2 or ridge regularization to calculate the loss on \n",
    "#both hidden layers. This is the loss calculated from our optimizer.\n",
    "#The total amount of loss will be the product of both types of loss.\n",
    "\n",
    "#Backpropogation:\n",
    "\n",
    "#For backpropogation, we need to apply the chain rule to every layer.\n",
    "#For this, we first start with the derivative of the BinaryCrossEntropy\n",
    "#with respect to the sigmoid activation output. Then, we take the \n",
    "#derivative of the sigmoid function with respect to the derivative\n",
    "#of the inputs of BinaryCrossEntropy. Next, we take the derivative\n",
    "#of the values of the input of sigmoid function. Then we take the \n",
    "#derivative of ReLU with respect to the derivative of the inputs \n",
    "#of the second hidden layer. We will then take the derivative of \n",
    "#the values and L2 regularization of weights and biases with respect to\n",
    "#the input of the derivative of ReLU.\n",
    "\n",
    "#We will then update our hyperparameters which are the weights and biases\n",
    "#along with the learning rate from our optimizer, Adam. This will be done\n",
    "#until we recieve an acceptable loss on our training, validation, and \n",
    "#testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_Layer:\n",
    "    \n",
    "    #initialize the layer\n",
    "    def __init__(self, neuron_inputs, neuron_neurons, L2_weight=0, L2_bias=0):\n",
    "        #initalize the variables\n",
    "        \n",
    "        #randomize the weights from a Gaussian Distribution with 0 as the\n",
    "        #mean and 1 as the variance\n",
    "        self.weights = 0.01 * np.random.randn(neuron_inputs, neuron_neurons)\n",
    "        self.biases = np.zeros((1, neuron_neurons))\n",
    "        \n",
    "        #initialize regularization for weight and bias\n",
    "        \n",
    "        self.L2_weight = L2_weight\n",
    "        self.L2_bias = L2_bias\n",
    "       \n",
    "        \n",
    "    #forward pass function for the first hidden layer                                  \n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        #saving the inputs for backwards pass\n",
    "        #the inputs of the forward pass is the input (data) itself\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #the output is the dot product of the inputs and weights \n",
    "        #plus the bias\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward_pass(self, dvalues):\n",
    "        \n",
    "        #calculate the gradients of weights, biases, and inputs\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        \n",
    "        #calculate the gradient on L2 regularization (derivative  of L2)\n",
    "        \n",
    "        if self.L2_weight > 0:\n",
    "            self.dweights += 2*self.L2_weight*self.weights\n",
    "            \n",
    "        if self.L2_bias > 0:\n",
    "            self.dbiases += 2*self.L2_bias*self.biases\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #forward pass using the output from the hidden layer as it's input\n",
    "    def forward_pass(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "    \n",
    "        #the ReLU activation function will return 0 if the input is smaller\n",
    "        #than 0, and the actual input if greater than 0\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        #all derivatives greater than 0 are equal to themselves\n",
    "        self.dinputs[self.inputs > 0 ] = self.dinputs[self.inputs>0]\n",
    "        \n",
    "        #all derivatives less than 0 are equal to 0\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def predictions(self, outputs):\n",
    "        return np.round(outputs)\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #self.inputs = inputs\n",
    "        self.outputs = 1 / (1+np.exp(-inputs))\n",
    "        \n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues * (1-self.outputs) * self.outputs\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def L2reg_loss(self, layer):\n",
    "        #initialize loss to 0\n",
    "        L2reg_loss = 0\n",
    "        \n",
    "        #changing the regularization loss if the weights \n",
    "        if layer.L2_weight >0:\n",
    "            L2reg_loss += layer.L2_weight* np.sum(layer.weights * layer.weights)\n",
    "        if layer.L2_bias >0:\n",
    "            L2reg_loss += layer.L2_bias*np.sum(layer.biases * layer.biases)\n",
    "                \n",
    "        return L2reg_loss\n",
    "                                              \n",
    "    \n",
    "    def calculate(self, outputs, y):\n",
    "        sample_loss = self.forward_pass(outputs, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "\n",
    "    \n",
    "class BinaryCrossEntropy(Loss):\n",
    "    def forward_pass(self, predicted_y, true_y):\n",
    "        predicted_y_clipped = np.clip(predicted_y, 1e-7, 1-1e-7)\n",
    "        sample_loss = -(true_y * np.log(predicted_y_clipped)+\n",
    "                        (1-true_y) * np.log(1-predicted_y_clipped))\n",
    "        sample_loss = np.mean(sample_loss, axis=-1)\n",
    "        return sample_loss\n",
    "    \n",
    "    def backward_pass(self, dvalues, true_y):\n",
    "        dvalues_clipped = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "        num_samples = len(dvalues)\n",
    "        num_outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -(true_y / dvalues_clipped - (1-true_y) / \\\n",
    "                         (1-dvalues_clipped)) / num_outputs\n",
    "        self.dinputs = self.dinputs / num_samples\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for optimizer. In this case we're using Adam\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0, decay=0, epsilon=0.00001, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon \n",
    "        self.b1 = b1 \n",
    "        self.b2 = b2\n",
    "        \n",
    "    #before we update parameters \n",
    "    def pre_parameter_update(self): \n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                        (1. / (1. + self.decay * self.iterations))\n",
    "    def parameter_update(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights) \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_momentum = np.zeros_like(layer.biases) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        \n",
    "        #calculate momentum for the bias and weight using beta 1\n",
    "        layer.weight_momentum = self.b1 *  layer.weight_momentum + \\\n",
    "                                (1 - self.b1) * layer.dweights \n",
    "        layer.bias_momentum = self.b1 * layer.bias_momentum + \\\n",
    "                                (1 - self.b1) * layer.dbiases\n",
    "        \n",
    "        #calculate cache for weight and bias using beta 2 (same as RMS prop)\n",
    "        layer.weight_cache = self.b2 * layer.weight_cache + \\\n",
    "                                (1 - self.b2) * (layer.dweights**2)\n",
    "        layer.bias_cache = self.b2 * layer.bias_cache + \\\n",
    "                                (1 - self.b2) * (layer.dbiases**2)\n",
    "    \n",
    "        #get corrected weight and bias momentum\n",
    "        weight_momentum_corrected = layer.weight_momentum / \\\n",
    "                                (1 - self.b1 ** (self.iterations + 1))\n",
    "        bias_momentum_corrected = layer.bias_momentum / \\\n",
    "                                (1 - self.b1 ** (self.iterations + 1))\n",
    "        \n",
    "        #get corrected weight and bias cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                (1 - self.b2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                                (1 - self.b2 ** (self.iterations + 1))\n",
    "        \n",
    "        #now we update the weights and bias\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                                weight_momentum_corrected / (np.sqrt(weight_cache_corrected) +\n",
    "                                                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                                bias_momentum_corrected / (np.sqrt(bias_cache_corrected) +\n",
    "                                                           self.epsilon)                 \n",
    "        \n",
    "    def post_parameter_update(self): \n",
    "        self.iterations = self.iterations + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1-rate\n",
    "    def forward_pass(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        self.outputs = inputs * self.binary_mask\n",
    "    def backward_pass(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.550, loss: 0.693, data_loss: 0.693, reg_loss: 0.000, lr: 0.0003\n",
      "epoch: 5, acc: 0.548, loss: 0.690, data_loss: 0.690, reg_loss: 0.000, lr: 0.00029998800047998075\n",
      "epoch: 10, acc: 0.548, loss: 0.688, data_loss: 0.688, reg_loss: 0.000, lr: 0.0002999730024297813\n",
      "epoch: 15, acc: 0.548, loss: 0.685, data_loss: 0.685, reg_loss: 0.000, lr: 0.0002999580058791769\n",
      "epoch: 20, acc: 0.548, loss: 0.681, data_loss: 0.681, reg_loss: 0.000, lr: 0.00029994301082794265\n",
      "epoch: 25, acc: 0.548, loss: 0.677, data_loss: 0.677, reg_loss: 0.001, lr: 0.00029992801727585374\n",
      "epoch: 30, acc: 0.548, loss: 0.673, data_loss: 0.673, reg_loss: 0.001, lr: 0.0002999130252226854\n",
      "epoch: 35, acc: 0.550, loss: 0.668, data_loss: 0.668, reg_loss: 0.001, lr: 0.0002998980346682128\n",
      "epoch: 40, acc: 0.554, loss: 0.662, data_loss: 0.662, reg_loss: 0.001, lr: 0.00029988304561221125\n",
      "epoch: 45, acc: 0.559, loss: 0.657, data_loss: 0.657, reg_loss: 0.001, lr: 0.000299868058054456\n",
      "epoch: 50, acc: 0.562, loss: 0.650, data_loss: 0.650, reg_loss: 0.001, lr: 0.0002998530719947225\n",
      "epoch: 55, acc: 0.585, loss: 0.643, data_loss: 0.643, reg_loss: 0.001, lr: 0.0002998380874327863\n",
      "epoch: 60, acc: 0.618, loss: 0.636, data_loss: 0.636, reg_loss: 0.001, lr: 0.0002998231043684226\n",
      "epoch: 65, acc: 0.637, loss: 0.628, data_loss: 0.628, reg_loss: 0.001, lr: 0.0002998081228014071\n",
      "epoch: 70, acc: 0.661, loss: 0.619, data_loss: 0.619, reg_loss: 0.002, lr: 0.0002997931427315152\n",
      "epoch: 75, acc: 0.696, loss: 0.611, data_loss: 0.611, reg_loss: 0.002, lr: 0.0002997781641585227\n",
      "epoch: 80, acc: 0.716, loss: 0.601, data_loss: 0.601, reg_loss: 0.002, lr: 0.000299763187082205\n",
      "epoch: 85, acc: 0.745, loss: 0.591, data_loss: 0.591, reg_loss: 0.002, lr: 0.000299748211502338\n",
      "epoch: 90, acc: 0.750, loss: 0.583, data_loss: 0.583, reg_loss: 0.003, lr: 0.00029973323741869733\n",
      "epoch: 95, acc: 0.766, loss: 0.573, data_loss: 0.573, reg_loss: 0.003, lr: 0.0002997182648310588\n",
      "epoch: 100, acc: 0.766, loss: 0.562, data_loss: 0.562, reg_loss: 0.003, lr: 0.0002997032937391982\n",
      "epoch: 105, acc: 0.766, loss: 0.554, data_loss: 0.554, reg_loss: 0.003, lr: 0.0002996883241428914\n",
      "epoch: 110, acc: 0.769, loss: 0.543, data_loss: 0.543, reg_loss: 0.004, lr: 0.0002996733560419143\n",
      "epoch: 115, acc: 0.772, loss: 0.535, data_loss: 0.535, reg_loss: 0.004, lr: 0.0002996583894360429\n",
      "epoch: 120, acc: 0.784, loss: 0.526, data_loss: 0.526, reg_loss: 0.004, lr: 0.00029964342432505317\n",
      "epoch: 125, acc: 0.783, loss: 0.519, data_loss: 0.519, reg_loss: 0.005, lr: 0.0002996284607087212\n",
      "epoch: 130, acc: 0.795, loss: 0.510, data_loss: 0.510, reg_loss: 0.005, lr: 0.000299613498586823\n",
      "epoch: 135, acc: 0.793, loss: 0.504, data_loss: 0.504, reg_loss: 0.006, lr: 0.0002995985379591348\n",
      "epoch: 140, acc: 0.790, loss: 0.496, data_loss: 0.496, reg_loss: 0.006, lr: 0.00029958357882543265\n",
      "epoch: 145, acc: 0.793, loss: 0.491, data_loss: 0.491, reg_loss: 0.006, lr: 0.0002995686211854928\n",
      "epoch: 150, acc: 0.798, loss: 0.483, data_loss: 0.483, reg_loss: 0.007, lr: 0.00029955366503909176\n",
      "epoch: 155, acc: 0.800, loss: 0.480, data_loss: 0.480, reg_loss: 0.007, lr: 0.0002995387103860055\n",
      "epoch: 160, acc: 0.809, loss: 0.473, data_loss: 0.473, reg_loss: 0.007, lr: 0.0002995237572260106\n",
      "epoch: 165, acc: 0.804, loss: 0.470, data_loss: 0.470, reg_loss: 0.007, lr: 0.00029950880555888336\n",
      "epoch: 170, acc: 0.798, loss: 0.465, data_loss: 0.465, reg_loss: 0.008, lr: 0.00029949385538440037\n",
      "epoch: 175, acc: 0.809, loss: 0.462, data_loss: 0.462, reg_loss: 0.008, lr: 0.0002994789067023379\n",
      "epoch: 180, acc: 0.809, loss: 0.458, data_loss: 0.458, reg_loss: 0.008, lr: 0.00029946395951247265\n",
      "epoch: 185, acc: 0.811, loss: 0.455, data_loss: 0.455, reg_loss: 0.009, lr: 0.0002994490138145811\n",
      "epoch: 190, acc: 0.807, loss: 0.453, data_loss: 0.453, reg_loss: 0.009, lr: 0.00029943406960844006\n",
      "epoch: 195, acc: 0.815, loss: 0.449, data_loss: 0.449, reg_loss: 0.009, lr: 0.00029941912689382595\n",
      "epoch: 200, acc: 0.812, loss: 0.448, data_loss: 0.448, reg_loss: 0.009, lr: 0.00029940418567051566\n",
      "epoch: 205, acc: 0.812, loss: 0.444, data_loss: 0.444, reg_loss: 0.010, lr: 0.00029938924593828585\n",
      "epoch: 210, acc: 0.818, loss: 0.441, data_loss: 0.441, reg_loss: 0.010, lr: 0.00029937430769691345\n",
      "epoch: 215, acc: 0.818, loss: 0.441, data_loss: 0.441, reg_loss: 0.010, lr: 0.00029935937094617516\n",
      "validation, acc: 0.826,  loss: 0.447\n",
      "test, acc: 0.862,  loss: 0.422\n"
     ]
    }
   ],
   "source": [
    "#create a hidden layer. Given 13 features we will produce 14 outputs(neurons)\n",
    "first_hidden = Hidden_Layer(13, 340, L2_weight=0.001, L2_bias=0.001)\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "#print(y_train)\n",
    "\n",
    "#initialize ReLU activation\n",
    "ReLU_activation = ReLU()\n",
    "\n",
    "first_dropout = Dropout(0.15)\n",
    "\n",
    "#create a second hidden layer\n",
    "second_hidden = Hidden_Layer(340, 1)\n",
    "\n",
    "#initialize sigmoid\n",
    "Sigmoid_activation = Sigmoid()\n",
    "\n",
    "#initialize loss function\n",
    "BinaryCrossEntropyloss = BinaryCrossEntropy()\n",
    "\n",
    "#initialize optimizing function\n",
    "Adam_optimizer = Adam(learning_rate=0.0003, decay=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(220):\n",
    "\n",
    "        #we perform the forward pass on the training data\n",
    "        first_hidden.forward_pass(df6_train_scaled)\n",
    "\n",
    "        #forward pass the outputs of the first layer to ReLU\n",
    "        ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "        \n",
    "        first_dropout.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "        #forward pass the outputs of ReLU in the second layer\n",
    "        second_hidden.forward_pass(first_dropout.outputs)\n",
    "        \n",
    "        #forward pass the outputs of the second hidden layer to sigmoid\n",
    "        Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "        \n",
    "        #calculate loss on forward pass\n",
    "        loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_train)\n",
    "        \n",
    "        #calculate regularization loss\n",
    "    \n",
    "        \n",
    "        L2reg_loss = \\\n",
    "            BinaryCrossEntropyloss.L2reg_loss(first_hidden) + BinaryCrossEntropyloss.L2reg_loss(second_hidden)\n",
    "        total_loss = loss + L2reg_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        #compare outputs from softmax to y for accuracy\n",
    "        probabilities =  Sigmoid_activation.outputs\n",
    "        predictions = np.round(probabilities)\n",
    "        \n",
    "        #predictions = (Sigmoid_activation.outputs > 0.5) *  1\n",
    "        accuracy = np.mean(predictions == y_train)\n",
    "        \n",
    "        if not epoch % 5: \n",
    "            print(f'epoch: {epoch}, ' +\n",
    "                  f'acc: {accuracy:.3f}, ' +\n",
    "                  f'loss: {loss:.3f}, ' +\n",
    "                  f'data_loss: {loss:.3f}, ' +\n",
    "                  f'reg_loss: {L2reg_loss:.3f}, ' +\n",
    "                  f'lr: {Adam_optimizer.current_learning_rate}')\n",
    "            \n",
    "        #THE CHAIN RULE APPLIED HERE!!!\n",
    "\n",
    "        #backwards pass the loss_activation output through the loss activation backward\n",
    "        BinaryCrossEntropyloss.backward_pass(Sigmoid_activation.outputs, y_train)\n",
    "\n",
    "        Sigmoid_activation.backward_pass(BinaryCrossEntropyloss.dinputs)\n",
    "\n",
    "        #backwards pass the derivative of the loss activation inputs through\n",
    "        #the backward function of dense2\n",
    "        second_hidden.backward_pass(Sigmoid_activation.dinputs)\n",
    "        \n",
    "        first_dropout.backward_pass(second_hidden.dinputs)\n",
    "\n",
    "\n",
    "        #backwards pass the derivative of the dense2 derivative inputs\n",
    "        #through the backward function of first activation function\n",
    "        ReLU_activation.backward_pass(first_dropout.dinputs)\n",
    "     \n",
    "        #backwards pass the derivative of the first activation function inputs\n",
    "        #through the backwards function of dense1\n",
    "        first_hidden.backward_pass(ReLU_activation.dinputs)\n",
    "            \n",
    "        \n",
    "        Adam_optimizer.pre_parameter_update()\n",
    "        Adam_optimizer.parameter_update(first_hidden)\n",
    "        Adam_optimizer.parameter_update(second_hidden) \n",
    "        Adam_optimizer.post_parameter_update()\n",
    "        \n",
    "\n",
    "y_val = y_val.reshape(-1,1) \n",
    "\n",
    "first_hidden.forward_pass(df6_val_scaled)\n",
    "\n",
    "ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "\n",
    "second_hidden.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_val)\n",
    "\n",
    "#compare outputs from softmax to y for accuracy\n",
    "probabilities =  Sigmoid_activation.outputs\n",
    "predictions = np.round(probabilities)\n",
    "\n",
    "accuracy = np.mean(predictions == y_val)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f},  loss: {loss:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "y_test = y_test.reshape(-1,1) \n",
    "\n",
    "first_hidden.forward_pass(df6_test_scaled)\n",
    "\n",
    "ReLU_activation.forward_pass(first_hidden.outputs)\n",
    "\n",
    "second_hidden.forward_pass(ReLU_activation.outputs)\n",
    "\n",
    "Sigmoid_activation.forward_pass(second_hidden.outputs)\n",
    "\n",
    "loss = BinaryCrossEntropyloss.calculate(Sigmoid_activation.outputs, y_test)\n",
    "\n",
    "#compare outputs from softmax to y for accuracy\n",
    "probabilities =  Sigmoid_activation.outputs\n",
    "predictions = np.round(probabilities)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'test, acc: {accuracy:.3f},  loss: {loss:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
